{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module used to connect Python with MongoDb\n",
    "import pymongo\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "from IPython.display import HTML\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default port used by MongoDB is 27017\n",
    "# https://docs.mongodb.com/manual/reference/default-mongodb-port/\n",
    "conn = 'mongodb://localhost:27017'\n",
    "client = pymongo.MongoClient(conn)\n",
    "\n",
    "# Define the 'classDB' database in Mongo\n",
    "db = client.classDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what we're going to do is we're going to scrape a page from analytics.usa.gov, we're going to create an r variable and then we're going to say r is equal to urllib.request.urlopen and we're going to pass in our URL into our urlopen function here so we are going to create a string called https://analytics.usa.gov and what we want to do is go ahead and read data out from that webpage so we're going to call up the read method off of this whole thing. And so next let's just create a Soup variable and what we're going to do is call our BeautifulSoup constructor and we're going to pass in our r variable which is our data from the webpage and we're going to tell BeautifulSoup that we want it to be read in as lxml and we' just cal the type function to doiuble check the type of our Soup object which of course should be BeautifulSoup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scrape page from analytics.usa.gov\n",
    "r = urllib.request.urlopen('https://analytics.usa.gov/').read()\n",
    "soup = BeautifulSoup(r, \"lxml\")\n",
    "type(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay ya and it is. Just remember here you can use any web link you want, so you can basifcally scrape data from any webpage on the internet. Now I'm going to show you how to scrape a webpage and save your results. First let's start by printing out our Soup object and we're going to use the prettify function so it can add just a little bit of structure and makes it a bit easier to read. So we will call our print function and then we will pass in Soup.prettify and then let's just print out the first 100 characters and run this.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      " <!-- Initalize title and data source variables -->\n",
      " <head>\n",
      "  <!--\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(soup.prettify()[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we're seeing a very very tip top of the webpage that's located at the analytics.usa.gov URL.\n",
    "Next what we should do is us the find all function to find all the a tags and then retrieve the href values from within those. To do that we are going create a loop and we're going to say for link in our Soup object, so Soup we want to call the find all method we want to search that object and we're searching it for all a tags so we'll pass in a string that says and then for so for each of those links let's print the link. So we'll call the print function. We'll pass in link.gete and what we want to do is we want to get the href we want to get that extension what is the actual link itself. So we're going to create a string that reads href and then we will run this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n",
      "#explanation\n",
      "https://analytics.usa.gov/data/\n",
      "https://open.gsa.gov/api/dap/\n",
      "data/\n",
      "#top-pages-realtime\n",
      "#top-pages-7-days\n",
      "#top-pages-30-days\n",
      "https://analytics.usa.gov/data/live/all-pages-realtime.csv\n",
      "https://analytics.usa.gov/data/live/all-domains-30-days.csv\n",
      "https://www.digitalgov.gov/services/dap/\n",
      "https://www.digitalgov.gov/services/dap/common-questions-about-dap-faq/#part-4\n",
      "https://support.google.com/analytics/answer/2763052?hl=en\n",
      "https://analytics.usa.gov/data/live/second-level-domains.csv\n",
      "https://analytics.usa.gov/data/live/sites.csv\n",
      "mailto:DAP@support.digitalgov.gov\n",
      "https://analytics.usa.gov/data/\n",
      "https://open.gsa.gov/api/dap/\n",
      "mailto:DAP@support.digitalgov.gov\n",
      "https://github.com/GSA/analytics.usa.gov/issues\n",
      "https://github.com/GSA/analytics.usa.gov\n",
      "https://github.com/18F/analytics-reporter\n",
      "http://www.gsa.gov/\n",
      "https://www.digitalgov.gov/services/dap/\n",
      "https://cloud.gov/\n"
     ]
    }
   ],
   "source": [
    "#find all a tags create loop\n",
    "for link in soup.find_all('a'):\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And very cool, basically what this has done is it has gon through analytics.usa.gov, that webpage and it looped through all of the text on that page and basically printed out only the weblinks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wanted to see what that body of text actuallky looks like then you can use the get text method so let's just try that out now. We are gonna say print and we're going to pass in our Soup object and then we're going to call the get text method off of our Soup object. So we'll say get_text and then run this whole thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "analytics.usa.gov | The US government's web traffic.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "analytics.usa.gov\n",
      "              \n",
      "\n",
      "\n",
      "About this site\n",
      "Data | API\n",
      "\n",
      "\n",
      "\n",
      "Select an agency\n",
      "\n",
      "All Participating Websites\n",
      "Agency for International Development\n",
      "Department of Agriculture\n",
      "Department of Commerce\n",
      "Department of Defense\n",
      "Department of Education\n",
      "Department of Energy\n",
      "Department of Health and Human Services\n",
      "Department of Homeland Security\n",
      "Department of Housing and Urban Development\n",
      "Department of Justice\n",
      "Department of Labor\n",
      "Department of State\n",
      "Department of Transportation\n",
      "Department of Veterans Affairs\n",
      "Department of the Interior\n",
      "Department of the Treasury\n",
      "Environmental Protection Agency\n",
      "Executive Office of the President\n",
      "General Services Administration\n",
      "National Aeronautics and Space Administration\n",
      "National Archives and Records Administration\n",
      "National Science Foundation\n",
      "Nuclear Regulatory Commission\n",
      "Office of Personnel Management\n",
      "Postal Service\n",
      "Small Business Administration\n",
      "Social Security Administration\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "...\n",
      "people on government websites now\n",
      "\n",
      "\n",
      "Visits Today\n",
      "Eastern Time\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Visits in the Past 90 Days\n",
      "\n",
      "\n",
      "          There were ... visits over the past 90 days.\n",
      "        \n",
      "\n",
      "Devices\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            Based on rough network segmentation data, we estimate that less than 5% of all traffic across all agencies comes from US federal government networks.\n",
      "          \n",
      "\n",
      "            Much more detailed data is available in downloadable CSV and JSON. This includes data on combined browser and OS usage.\n",
      "          \n",
      "\n",
      "\n",
      "Browsers\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Internet Explorer\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Operating Systems\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Windows\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Visitor Locations Right Now\n",
      "\n",
      "\n",
      "Cities\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Countries\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "United States & Territories\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "International\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Top Pages\n",
      "\n",
      "Now\n",
      "7 Days\n",
      "30 Days\n",
      "\n",
      "\n",
      "\n",
      "              People on a single, specific page now. We only count pages with at least 10 people on the page.\n",
      "              Download the full dataset.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Visits over the last week to domains, including traffic to all pages within that domain.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "              Visits over the last month to domains, including traffic to all pages within that domain. We only count pages with at least 1,000 visits in the last month.\n",
      "              Download the full dataset.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Top Downloads\n",
      "Total file downloads yesterday on government domains.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "About this Site\n",
      "\n",
      "            These data provide a window into how people are interacting with the government online.\n",
      "             The data come from a unified Google Analytics account for U.S. federal government agencies known as the Digital Analytics Program.\n",
      "              This program helps government agencies understand how people find, access, and use government services online. The program does not track individuals,\n",
      "               and anonymizes the IP addresses of visitors.\n",
      "          \n",
      "\n",
      "            Not every government website is represented in these data. \n",
      "            Currently, the Digital Analytics Program collects web traffic from around 400 executive branch government domains,\n",
      "             across about 5,700 total websites,\n",
      "              including every cabinet department.\n",
      "               We continue to pursue and add more sites frequently; to add your site, email the Digital Analytics Program.\n",
      "          \n",
      "\n",
      "\n",
      "Download the data\n",
      "You can download the data here. Available in JSON and CSV format.\n",
      " Additionally, you can access data via our  API project (currently in Beta).\n",
      "A note on sampling\n",
      "Due to varying Google Analytics API sampling thresholds and the sheer volume of data in this project, some non-realtime reports may be subject to sampling. \n",
      "             The data are intended to represent trends and numbers may not be precise.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Have a question or problem? \n",
      "              \n",
      "              Get in touch.\n",
      "              \n",
      "\n",
      "\n",
      "                  Suggest a feature or report an issue\n",
      "            \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "              View our code on GitHub\n",
      "\n",
      "\n",
      "              View our code for the data on GitHub\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Analytics.usa.gov is a project of GSAâ€™s Digital Analytics Program.\n",
      "This website is hosted on cloud.gov.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#to see what the body of text looks like, call the get text method, run the whole thing all the text found on the page\n",
    "print(soup.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can kind of just peruse this real quick. This is basically all the text that is found on that page itself, and so you know there's a lot of stuff on here but what Python did for us was nice. Because it really condensed down all of this text into what we needed for web links right? So that's convenient. \n",
    "Let's go ahead and prettify this so we can kind of look at it and understand it more easily. To do that we'll just say print and we'll pass i nour Soup object Soup.prettify and then lets just look at the first 1,000 characcters, so to do that we will say select only zero through 1000 okay nad run this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      " <!-- Initalize title and data source variables -->\n",
      " <head>\n",
      "  <!--\n",
      "\n",
      "    Hi! Welcome to our source code.\n",
      "\n",
      "    This dashboard uses data from the Digital Analytics Program, a US\n",
      "    government team inside the General Services Administration.\n",
      "\n",
      "\n",
      "    For a detailed tech breakdown of how 18F and friends built this site:\n",
      "\n",
      "    https://18f.gsa.gov/2015/03/19/how-we-built-analytics-usa-gov/\n",
      "\n",
      "\n",
      "    This is a fully open source project, and your contributions are welcome.\n",
      "\n",
      "    Frontend static site: https://github.com/18F/analytics.usa.gov\n",
      "    Backend data reporting: https://github.com/18F/analytics-reporter\n",
      "\n",
      "    -->\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <meta content=\"IE=Edge\" http-equiv=\"X-UA-Compatible\"/>\n",
      "  <meta content=\"NjbZn6hQe7OwV-nTsa6nLmtrOUcSGPRyFjxm5zkmCcg\" name=\"google-site-verification\"/>\n",
      "  <link href=\"/css/vendor/css/uswds.v0.9.6.css\" rel=\"stylesheet\"/>\n",
      "  <link href=\"/css/public_analytics.css\" rel=\"stylesheet\"/>\n",
      "  <link href=\"/images/analytics-favicon.ico\" rel=\"ic\n"
     ]
    }
   ],
   "source": [
    "#prettify it; pring and pass in soup object and look only at the first 1k characters\n",
    "print(soup.prettify()[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create for loop to pass through our Soup object and find all of the a tags that have an atribute of href. We start by creating a forloop for every link in our soup object we want to use the find all method, we want to find all a tags with attibute equal to href, and then for all of these tags that are returned we want the loop to match against them a regular expression that reads http and print out only those.  So to make that happen we will say re.compile call the compile function and then we will pass the tag against which our results should be matched.  So we are going to say http and then for any results that match the expression we just print them out. So we say print link.Then we will look what is the type for this link.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"https://analytics.usa.gov/data/\">Data</a>\n",
      "<a href=\"https://open.gsa.gov/api/dap/\" rel=\"noopener\" target=\"_blank\">API</a>\n",
      "<a href=\"https://analytics.usa.gov/data/live/all-pages-realtime.csv\">Download the full dataset.</a>\n",
      "<a href=\"https://analytics.usa.gov/data/live/all-domains-30-days.csv\">Download the full dataset.</a>\n",
      "<a class=\"external-link\" href=\"https://www.digitalgov.gov/services/dap/\">Digital Analytics Program</a>\n",
      "<a class=\"external-link\" href=\"https://www.digitalgov.gov/services/dap/common-questions-about-dap-faq/#part-4\">does not track individuals</a>\n",
      "<a class=\"external-link\" href=\"https://support.google.com/analytics/answer/2763052?hl=en\">anonymizes the IP addresses</a>\n",
      "<a class=\"external-link\" href=\"https://analytics.usa.gov/data/live/second-level-domains.csv\">400 executive branch government domains</a>\n",
      "<a class=\"external-link\" href=\"https://analytics.usa.gov/data/live/sites.csv\">about 5,700 total websites</a>\n",
      "<a href=\"https://analytics.usa.gov/data/\">download the data here.</a>\n",
      "<a href=\"https://open.gsa.gov/api/dap/\" rel=\"noopener\" target=\"_blank\"> API project</a>\n",
      "<a class=\"usa-button usa-button-secondary-inverse\" href=\"https://github.com/GSA/analytics.usa.gov/issues\">\n",
      "<img alt=\"Github Icon\" class=\"github-icon\" src=\"/images/github-logo-white.svg\"/>\n",
      "                  Suggest a feature or report an issue\n",
      "            </a>\n",
      "<a href=\"https://github.com/GSA/analytics.usa.gov\">\n",
      "<img alt=\"Github Icon\" class=\"github-icon\" src=\"/images/github-logo.svg\"/>\n",
      "              View our code on GitHub</a>\n",
      "<a href=\"https://github.com/18F/analytics-reporter\">\n",
      "<img alt=\"Github Icon\" class=\"github-icon\" src=\"/images/github-logo.svg\"/>\n",
      "              View our code for the data on GitHub</a>\n",
      "<a href=\"http://www.gsa.gov/\">\n",
      "<img alt=\"GSA\" src=\"/images/gsa-logo.svg\"/>\n",
      "</a>\n",
      "<a href=\"https://www.digitalgov.gov/services/dap/\">Digital Analytics Program</a>\n",
      "<a href=\"https://cloud.gov/\">cloud.gov</a>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "bs4.element.Tag"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for link in soup.findAll('a', attrs={'href': re.compile(\"^http\")}):\n",
    "    print(link)\n",
    "type(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the link here is a Tag object and what we basically have is we  have all of our a tags that have an attribute of href and also have an http match within them.  It isn't useful for you to have this result stuck within Jupyter notebook though so you will want to know how to save this as an external file.  To do that we're going to create a new text file called parse data, and so were' going to create file variable and we are going to say, we want to open a new file called parsed_data.txt it's going to ba a text file and we want to write into that text file. So we're going to pass in a w parameter and then what we want to do is for each of the links that was just printed out we want to print those now into the parsed_data text file. So we will copy the code from above: for link in soup.findAll('a', attrs={'href': re.compile(\"^http\")}):\n",
    "    print(link) \n",
    "and it's performaing the same operations now as it did before but instead of printing here into jupyter, it's going to go ahead and it's going to generate a Soup link and thats going to be a string. So instead of printing the link it's going to create a new variable called soup_link and that link is going to be equal to a string and this string is going to be derived from each link within the Soup object, and so for each soup_link we just want to print that out. So we are going to say print soup_link and we are going to write it into the file so we're going to say file.write, we're going to pass in what we want written which is going to be the soup_link and what this loop is going to do is it's going to pass it the entiore BeautifulSoup object and it's going to find all of the links and print them out until it finds no more links and then it is going to flush the file and close the file. To make that happen we are going to say file.flush() and file.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"https://analytics.usa.gov/data/\">Data</a>\n",
      "<a href=\"https://open.gsa.gov/api/dap/\" rel=\"noopener\" target=\"_blank\">API</a>\n",
      "<a href=\"https://analytics.usa.gov/data/live/all-pages-realtime.csv\">Download the full dataset.</a>\n",
      "<a href=\"https://analytics.usa.gov/data/live/all-domains-30-days.csv\">Download the full dataset.</a>\n",
      "<a class=\"external-link\" href=\"https://www.digitalgov.gov/services/dap/\">Digital Analytics Program</a>\n",
      "<a class=\"external-link\" href=\"https://www.digitalgov.gov/services/dap/common-questions-about-dap-faq/#part-4\">does not track individuals</a>\n",
      "<a class=\"external-link\" href=\"https://support.google.com/analytics/answer/2763052?hl=en\">anonymizes the IP addresses</a>\n",
      "<a class=\"external-link\" href=\"https://analytics.usa.gov/data/live/second-level-domains.csv\">400 executive branch government domains</a>\n",
      "<a class=\"external-link\" href=\"https://analytics.usa.gov/data/live/sites.csv\">about 5,700 total websites</a>\n",
      "<a href=\"https://analytics.usa.gov/data/\">download the data here.</a>\n",
      "<a href=\"https://open.gsa.gov/api/dap/\" rel=\"noopener\" target=\"_blank\"> API project</a>\n",
      "<a class=\"usa-button usa-button-secondary-inverse\" href=\"https://github.com/GSA/analytics.usa.gov/issues\">\n",
      "<img alt=\"Github Icon\" class=\"github-icon\" src=\"/images/github-logo-white.svg\"/>\n",
      "                  Suggest a feature or report an issue\n",
      "            </a>\n",
      "<a href=\"https://github.com/GSA/analytics.usa.gov\">\n",
      "<img alt=\"Github Icon\" class=\"github-icon\" src=\"/images/github-logo.svg\"/>\n",
      "              View our code on GitHub</a>\n",
      "<a href=\"https://github.com/18F/analytics-reporter\">\n",
      "<img alt=\"Github Icon\" class=\"github-icon\" src=\"/images/github-logo.svg\"/>\n",
      "              View our code for the data on GitHub</a>\n",
      "<a href=\"http://www.gsa.gov/\">\n",
      "<img alt=\"GSA\" src=\"/images/gsa-logo.svg\"/>\n",
      "</a>\n",
      "<a href=\"https://www.digitalgov.gov/services/dap/\">Digital Analytics Program</a>\n",
      "<a href=\"https://cloud.gov/\">cloud.gov</a>\n"
     ]
    }
   ],
   "source": [
    "file = open(\"parsed_data.txt\", \"w\")\n",
    "for link in soup.findAll('a', attrs={'href': re.compile(\"^http\")}):\n",
    "    soup_link = str(link)\n",
    "    print(soup_link)\n",
    "    file.write(soup_link)\n",
    "file.flush()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to pull up your present working directory you type %pwd this way we can check to see where that file was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\mbannon\\\\Desktop\\\\WebScraping\\\\web-scraping-challenge\\\\Missions_to_Mars'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
